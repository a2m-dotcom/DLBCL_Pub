{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmwu6fSgcG/NXDOE/6uMZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a2m-dotcom/DLBCL_Pub/blob/main/Mask_and_Crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H4Fz5V9Fcz5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKV024k-ZjI7"
      },
      "outputs": [],
      "source": [
        "df = df_clean.copy()  # from previous step\n",
        "\n",
        "images_dir = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/images\"\n",
        "crops_dir  = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/crops\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(crops_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "marydFNWZruA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "CROP_SIZE = 128\n",
        "HALF = CROP_SIZE // 2\n",
        "\n",
        "def safe_crop(img, x, y, crop_size=128):\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    x1 = max(0, x - HALF)\n",
        "    y1 = max(0, y - HALF)\n",
        "    x2 = min(w, x + HALF)\n",
        "    y2 = min(h, y + HALF)\n",
        "\n",
        "    crop = img[y1:y2, x1:x2]\n",
        "\n",
        "    # If crop is smaller at boundaries â†’ pad to 128px\n",
        "    if crop.shape[0] != CROP_SIZE or crop.shape[1] != CROP_SIZE:\n",
        "        pad_y = CROP_SIZE - crop.shape[0]\n",
        "        pad_x = CROP_SIZE - crop.shape[1]\n",
        "        crop = np.pad(crop, ((0,pad_y),(0,pad_x),(0,0)), mode='constant', constant_values=0)\n",
        "\n",
        "    return crop\n"
      ],
      "metadata": {
        "id": "J0FqFU2VZsQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "CROP_SIZE = 128\n",
        "HALF = CROP_SIZE // 2\n",
        "\n",
        "def safe_crop(img, x, y):\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    x1 = max(0, x - HALF)\n",
        "    y1 = max(0, y - HALF)\n",
        "    x2 = min(w, x + HALF)\n",
        "    y2 = min(h, y + HALF)\n",
        "\n",
        "    crop = img[y1:y2, x1:x2]\n",
        "\n",
        "    if crop.shape[0] != CROP_SIZE or crop.shape[1] != CROP_SIZE:\n",
        "        pad_y = CROP_SIZE - crop.shape[0]\n",
        "        pad_x = CROP_SIZE - crop.shape[1]\n",
        "        crop = np.pad(crop, ((0, pad_y), (0, pad_x), (0, 0)),\n",
        "                      mode='constant', constant_values=0)\n",
        "\n",
        "    return crop\n",
        "\n",
        "\n",
        "images_dir = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/images\"\n",
        "crops_dir  = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/crops\"\n",
        "os.makedirs(crops_dir, exist_ok=True)\n",
        "\n",
        "# GROUP BY SLIDE\n",
        "grouped = df.groupby(\"filename\")\n",
        "\n",
        "for slide_name, group in tqdm(grouped, total=len(grouped)):\n",
        "    slide_path = f\"{images_dir}/{slide_name}\"\n",
        "\n",
        "    img = cv2.imread(slide_path)\n",
        "    if img is None:\n",
        "        print(\"Could not open:\", slide_path)\n",
        "        continue\n",
        "\n",
        "    # Process all annotations belonging to THIS slide\n",
        "    for _, row in group.iterrows():\n",
        "        x = int(row[\"x\"])\n",
        "        y = int(row[\"y\"])\n",
        "        ann = int(row[\"annotationID\"])\n",
        "\n",
        "        crop = safe_crop(img, x, y)\n",
        "        crop_path = f\"{crops_dir}/{ann}.png\"\n",
        "        cv2.imwrite(crop_path, crop)\n",
        "\n"
      ],
      "metadata": {
        "id": "BFLgtZnCZuzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ChaoningZhang/MobileSAM.git"
      ],
      "metadata": {
        "id": "WKxF-gspZxoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python matplotlib\n"
      ],
      "metadata": {
        "id": "FCP5UF1VZ8Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O sam_vit_b.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n"
      ],
      "metadata": {
        "id": "9P1WeuTMaGiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam_checkpoint = \"sam_vit_b.pth\"\n",
        "model_type = \"vit_b\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "8AE0mmZEaIsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def get_sam_mask(crop):\n",
        "    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
        "    predictor.set_image(crop_rgb)\n",
        "\n",
        "    input_point = np.array([[64, 64]])\n",
        "    input_label = np.array([1])\n",
        "\n",
        "    masks, scores, _ = predictor.predict(\n",
        "        point_coords=input_point,\n",
        "        point_labels=input_label,\n",
        "        multimask_output=False\n",
        "    )\n",
        "\n",
        "    mask = (masks[0] * 255).astype(np.uint8)\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "arhBHFJ8aNu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean[\"crop_path\"] = df_clean[\"annotationID\"].apply(\n",
        "    lambda x: f\"{crops_dir}/{x}.png\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "69icOO8BaP2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for idx, row in tqdm(df_clean.iterrows(), total=len(df_clean)):\n",
        "    crop_path = row[\"crop_path\"]\n",
        "    ann = int(row[\"annotationID\"])\n",
        "\n",
        "    crop = cv2.imread(crop_path)\n",
        "    if crop is None:\n",
        "        print(\"Failed to read:\", crop_path)\n",
        "        continue\n",
        "\n",
        "    mask = get_sam_mask(crop)\n",
        "    cv2.imwrite(f\"{masks_dir}/{ann}.png\", mask)"
      ],
      "metadata": {
        "id": "pfavSvqcaRm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/images\"\n",
        "crops_dir  = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/crops\"\n",
        "masks_dir = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/masks\""
      ],
      "metadata": {
        "id": "6znCxJMlaWIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "images_dir = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/images\"\n",
        "crops_dir  = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/crops\"\n",
        "masks_dir  = \"/content/drive/MyDrive/MIDOGPP/MIDOGpp/masks\"\n",
        "\n",
        "def list_files(path):\n",
        "    return sorted([f for f in os.listdir(path) if not f.startswith(\".\")])\n",
        "\n",
        "crop_files = list_files(crops_dir)\n",
        "mask_files = list_files(masks_dir)\n",
        "image_files = list_files(images_dir)\n",
        "\n",
        "# Extract annotation_id from crop files (remove extension)\n",
        "anno_ids = [os.path.splitext(f)[0] for f in crop_files]\n",
        "\n",
        "rows = []\n",
        "\n",
        "for anno in anno_ids:\n",
        "    crop_path = os.path.join(crops_dir, anno + \".png\")\n",
        "    if not os.path.exists(crop_path):\n",
        "        crop_path = os.path.join(crops_dir, anno + \".jpg\")\n",
        "        if not os.path.exists(crop_path):\n",
        "            continue\n",
        "\n",
        "    mask_path = os.path.join(masks_dir, anno + \".png\")\n",
        "    if not os.path.exists(mask_path):\n",
        "        mask_path = os.path.join(masks_dir, anno + \".jpg\")\n",
        "        if not os.path.exists(mask_path):\n",
        "            continue\n",
        "\n",
        "    # determine parent image\n",
        "    # Usually annotation filename contains original patch/image ID\n",
        "    image_id = anno.split(\"_\")[0]   # IMPORTANT: adjust if needed\n",
        "\n",
        "    # find matching image file\n",
        "    possible_imgs = [\n",
        "        os.path.join(images_dir, image_id + ext)\n",
        "        for ext in [\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\"]\n",
        "    ]\n",
        "\n",
        "    image_path = None\n",
        "    for p in possible_imgs:\n",
        "        if os.path.exists(p):\n",
        "            image_path = p\n",
        "            break\n",
        "\n",
        "    if image_path is None:\n",
        "        continue\n",
        "\n",
        "    rows.append([anno, image_path, crop_path, mask_path])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"annotation_id\", \"image_path\", \"crop_path\", \"mask_path\"])\n",
        "\n",
        "print(\"Total annotations:\", len(anno_ids))\n",
        "print(\"Usable complete samples:\", len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SuzgktjZadaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}